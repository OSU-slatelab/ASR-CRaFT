#!/bin/bash

exp_dir= $1

###################################################
# step 0: prepare feature files in pfile format
###################################################
 
# You can use your favourite tool to train the DNN features for training and test sets, e.g.
# with QuickNet, Kaldi or Theano. But the feature files need to be converted to the pfile
# format defined by QuickNet before they are fed into CRF. Here we provide an example to
# convert the Kaldi DNN posteriors to the pfile format.

cd @timitdir@

./run.sh
./mylocal/run_dnn_fmllr_mono.sh --skip_smbr false

# Then generate Kaldi DNN features and also save them in HTK format:
# (The DNN features could be posteriors or activations of any intermediate layers)

./mylocal/prep_bn_feats.sh \
  --srcdata data-fmllr-tri3 \
  --dnndir exp/dnn_fmllr_mono_pretrain-dbn_dnn \
  --data_bn data-bn-dnn-fmllr-mono

# --data_bn defines the output directory, which is also the input directory to the next script below.

# Then convert features in HTK format to pfile format and generate the training and test feature files,
# which are used by QuickNet and our ASRCRaFT:

mkdir -p data-pf

./mylocal/htk_to_pf.sh \
  data-bn-dnn-fmllr-mono \
  data-pf/timit-train.pf \
  data-pf/timit-test.pf

# Note the training utterances order is hard-coded in htk_to_pf.sh by mylocal/timit_sisx_train.olist.kaldi_to_htk_to_pfile,
# which follows the same order in the training label files provided in timit-aux, timit_train.48labs.ilab and timit_sisx_train.olist

# The generated feature files are used in the config files in step 1 below.

###################################################
# step 1: create experiment config file
###################################################

# Example for SCRF (with transition features) on TIMIT:
# segmental-timit-demo.cfg

# Note that the second feature stream in the segmental config file
# is generated from the first feature stream by padding 6 frames at the
# beginning and the end so that the boundary feature can be easily
# generated because the current feature context window at the boundary
# considers 6 frames to the left and 6 frames to the right. The padding
# is done by running the following command (Note: installation should
# have installed feacat to your PATH) :

feacat -i data-pf/timit-train.pf -o data-pf/timit-train.pad6.pf -p 6
feacat -i data-pf/timit-test.pf -o data-pf/timit-test.pad6.pf -p 6

###################################################
# step 2: prepare your experiment directory
###################################################

# This creates the directory named in the input argument to this
# script, and in that directory creates a symlink to the directory
# containing the built and installed scripts referenced below.

@pkglibexecdir@/scrf-scripts/prep_exp.sh \
  @pkgdata@/segmental-timit-demo.cfg \
  $exp_dir

###################################################
# step 3: training
###################################################

cd $exp_dir
scripts/run_crf_train.sh lr=<learning rate> #& #run in bkg to speed up decode?

# Run @pkglibexecdir@/scrf-scripts/run_crf_train.sh for usage.

# The training will keep running until it finishes the number
# of iterations defined as ITERS in the config file.

# Learning rate is a critical hyper-parameter, which needs to be tuned.

# The (roughly) optimal learning rates tuned for TIMIT
# with SCRF (as configured in segmental-timit-demo.cfg): 0.1

###################################################
# step 4: decoding
###################################################

scripts/run_crf_fstdecode.sh lr=<learning rate>

# It can be run in parallel with training. By default it will decode
# with all available iterations so far (skipping those iterations
# previously decoded).

# You could also specify the iterations to decode, eg:

# scripts/run_crf_fstdecode.sh lr=<learning rate> \
# [start=<start iter>] [step=<step>] [end=<end iter>]

# e.g.
# scripts/run_crf_fstdecode.sh lr=0.1 start=3
# scripts/run_crf_fstdecode.sh lr=0.1 end=7
# scripts/run_crf_fstdecode.sh lr=0.1 start=3 end=7
# scripts/run_crf_fstdecode.sh lr=0.1 start=3 step=1 end=7

# The test sets can also be specified by the "testsets=" argument.

# Run scripts/run_crf_fstdecode.latest.sh for usage.

###################################################
# step 5: result
###################################################

scripts/sortResult.kaldi.sh

# This will collect all decoded results.
# The learning rate specific results are put, e.g. in
# weights.lr0.1/{cv,core,enh}_results.all.txt
# All results across learning rates are put in
# weights.all_lr_eta.{cv,core,enh}_results.all.txt

# Choose the best iteration based on the dev test performance, then
# report the core (or enhanced) test performance corresponding to
# that iteration.

# Detailed result report is in, e.g.:

# weights.lr0.1/cv_out.lr0.1/3iters/ctm_39phn.filt.sys
